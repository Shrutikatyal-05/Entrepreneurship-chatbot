{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit -q\n",
        "!pip install pyngrok\n",
        "!python setup.py bdist_wheel\n",
        "!pip install indic-nlp-library\n",
        "!pip install mosestokenizer\n",
        "!pip install unidecode\n",
        "!pip install pydload\n",
        "!pip install -e .\n",
        "!pip install langchain==0.0.166\n",
        "!pip install sentence_transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install openai\n",
        "!pip install ai4bharat-transliteration"
      ],
      "metadata": {
        "id": "Mtwl3USs4edw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccca3741-8a93-4dff-fdf9-041f6bfc0863"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n",
            "python3: can't open file '/content/setup.py': [Errno 2] No such file or directory\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.1)\n",
            "Collecting sphinx>=5.1.0 (from sphinx-argparse->indic-nlp-library)\n",
            "  Downloading sphinx-8.0.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting docutils>=0.19 (from sphinx-argparse->indic-nlp-library)\n",
            "  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting sphinx>=5.1.0 (from sphinx-argparse->indic-nlp-library)\n",
            "  Downloading sphinx-7.4.7-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting docutils>=0.19 (from sphinx-argparse->indic-nlp-library)\n",
            "  Downloading docutils-0.20.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.4)\n",
            "Collecting Pygments>=2.17 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
            "  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.16.0)\n",
            "Requirement already satisfied: alabaster~=0.7.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (0.7.16)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.1)\n",
            "Requirement already satisfied: tomli>=2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2024.8.30)\n",
            "Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m944.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
            "Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docutils-0.20.1-py3-none-any.whl (572 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.7/572.7 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx-7.4.7-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: morfessor, Pygments, docutils, sphinx, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "  Attempting uninstall: Pygments\n",
            "    Found existing installation: Pygments 2.16.1\n",
            "    Uninstalling Pygments-2.16.1:\n",
            "      Successfully uninstalled Pygments-2.16.1\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.18.1\n",
            "    Uninstalling docutils-0.18.1:\n",
            "      Successfully uninstalled docutils-0.18.1\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 5.0.2\n",
            "    Uninstalling Sphinx-5.0.2:\n",
            "      Successfully uninstalled Sphinx-5.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pygments-2.18.0 docutils-0.20.1 indic-nlp-library-0.92 morfessor-2.0.6 sphinx-7.4.7 sphinx-argparse-0.5.2 sphinx-rtd-theme-2.0.0 sphinxcontrib-jquery-4.1\n",
            "Collecting mosestokenizer\n",
            "  Downloading mosestokenizer-1.2.1.tar.gz (37 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from mosestokenizer)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openfile (from mosestokenizer)\n",
            "  Downloading openfile-0.0.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting uctools (from mosestokenizer)\n",
            "  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting toolwrapper (from mosestokenizer)\n",
            "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
            "Building wheels for collected packages: mosestokenizer, docopt, toolwrapper, uctools\n",
            "  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mosestokenizer: filename=mosestokenizer-1.2.1-py3-none-any.whl size=49172 sha256=a5c311cc6b0e4410a689e38824353ebf5698ff8f41ed1e00aaab65926a073ac0\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d8/15/4c5ebbe883513f003cb055a0369c77c9df857023a706f39e70\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=720774dbdf2546a3c4ed1b067bd865d0ee695495960ed349e286d83e661889b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3338 sha256=dfc889f8a23a2c62e331dff8f3ae3a8199233efa78325d00ef740d10970d981e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/af/b1/99b57a06dda78fdcee86d2e22c64743f3b8df8f31cfc04baf7\n",
            "  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6146 sha256=d1fe8a5a7bdb33b8a1578d2b779ad52be93a7205a13c03e5f9e90f6e09650680\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/ee/10/33257b0801ac6a912c841939032c16da1eb3db377afe1443e5\n",
            "Successfully built mosestokenizer docopt toolwrapper uctools\n",
            "Installing collected packages: toolwrapper, openfile, docopt, uctools, mosestokenizer\n",
            "Successfully installed docopt-0.6.2 mosestokenizer-1.2.1 openfile-0.0.7 toolwrapper-2.1.0 uctools-1.3.0\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n",
            "Collecting pydload\n",
            "  Downloading pydload-1.0.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.10/dist-packages (from pydload) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pydload) (2.32.3)\n",
            "Requirement already satisfied: python-utils>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from progressbar2->pydload) (3.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pydload) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pydload) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pydload) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pydload) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>3.10.0.2 in /usr/local/lib/python3.10/dist-packages (from python-utils>=3.0.0->progressbar2->pydload) (4.12.2)\n",
            "Downloading pydload-1.0.9-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: pydload\n",
            "Successfully installed pydload-1.0.9\n",
            "Obtaining file:///content\n",
            "\u001b[31mERROR: file:///content does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting langchain==0.0.166\n",
            "  Downloading langchain-0.0.166-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.166)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166) (2.10.1)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166) (1.26.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.166)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting pydantic<2,>=1 (from langchain==0.0.166)\n",
            "  Downloading pydantic-1.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.2/152.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166) (8.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.166)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.166)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.166) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.166) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.166) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.166) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.166) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.166) (3.0.3)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.166) (24.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.166)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain-0.0.166-py3-none-any.whl (803 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-1.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: pydantic, mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, dataclasses-json, langchain\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.8.2\n",
            "    Uninstalling pydantic-2.8.2:\n",
            "      Successfully uninstalled pydantic-2.8.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 1.4.14 requires pydantic>=2.7.0, but you have pydantic 1.10.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.5.14 langchain-0.0.166 marshmallow-3.22.0 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 pydantic-1.10.18 typing-inspect-0.9.0\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence_transformers\n",
            "Successfully installed sentence_transformers-3.0.1\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0.post1\n",
            "Collecting openai\n",
            "  Downloading openai-1.44.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.18)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading openai-1.44.0-py3-none-any.whl (367 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.8/367.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.44.0\n",
            "Collecting ai4bharat-transliteration\n",
            "  Downloading ai4bharat_transliteration-1.1.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydload in /usr/local/lib/python3.10/dist-packages (from ai4bharat-transliteration) (1.0.9)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from ai4bharat-transliteration) (2.2.5)\n",
            "Collecting flask-cors (from ai4bharat-transliteration)\n",
            "  Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting gevent (from ai4bharat-transliteration)\n",
            "  Downloading gevent-24.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting sacremoses (from ai4bharat-transliteration)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ai4bharat-transliteration) (2.1.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ai4bharat-transliteration) (4.66.5)\n",
            "Collecting ujson (from ai4bharat-transliteration)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting mock (from ai4bharat-transliteration)\n",
            "  Downloading mock-5.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting tensorboardX (from ai4bharat-transliteration)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from ai4bharat-transliteration) (14.0.2)\n",
            "Collecting fairseq (from ai4bharat-transliteration)\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting urduhack (from ai4bharat-transliteration)\n",
            "  Downloading urduhack-1.1.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.10/dist-packages (from ai4bharat-transliteration) (0.92)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq->ai4bharat-transliteration) (1.17.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq->ai4bharat-transliteration) (3.0.11)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq->ai4bharat-transliteration)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting omegaconf<2.1 (from fairseq->ai4bharat-transliteration)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.6 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl (from fairseq->ai4bharat-transliteration) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading omegaconf-2.0.5-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.5 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl (from fairseq->ai4bharat-transliteration) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading omegaconf-2.0.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.4 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/92/b1/4f3023143436f12c98bab53f0b3db617bd18a7d223627d5030e13a7b4fc2/omegaconf-2.0.4-py3-none-any.whl (from fairseq->ai4bharat-transliteration) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading omegaconf-2.0.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.3 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/29/08/a88210c2c1aa0a3f65f05d8a6c98939ccb84b6fb982aa6567dec4e6773f9/omegaconf-2.0.3-py3-none-any.whl (from fairseq->ai4bharat-transliteration) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading omegaconf-2.0.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.2 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/72/fe/f8d162aa059fb4f327fd75144dd69aa7e8acbb6d8d37013e4638c8490e0b/omegaconf-2.0.2-py3-none-any.whl (from fairseq->ai4bharat-transliteration) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading omegaconf-2.0.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.1 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/86/ec/605805e60abdb025b06664d107335031bb8ebdc52e0a90bdbad6a7130279/omegaconf-2.0.1-py3-none-any.whl (from fairseq->ai4bharat-transliteration) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading omegaconf-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq->ai4bharat-transliteration) (2024.5.15)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq->ai4bharat-transliteration)\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq->ai4bharat-transliteration) (2.4.0+cu121)\n",
            "Collecting bitarray (from fairseq->ai4bharat-transliteration)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq->ai4bharat-transliteration) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq->ai4bharat-transliteration) (1.26.4)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->ai4bharat-transliteration) (3.0.4)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->ai4bharat-transliteration) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->ai4bharat-transliteration) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->ai4bharat-transliteration) (8.1.7)\n",
            "Collecting zope.event (from gevent->ai4bharat-transliteration)\n",
            "  Downloading zope.event-5.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting zope.interface (from gevent->ai4bharat-transliteration)\n",
            "  Downloading zope.interface-7.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gevent->ai4bharat-transliteration) (3.0.3)\n",
            "Requirement already satisfied: sphinx-argparse in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library->ai4bharat-transliteration) (0.5.2)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library->ai4bharat-transliteration) (2.0.0)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library->ai4bharat-transliteration) (2.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ai4bharat-transliteration) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ai4bharat-transliteration) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ai4bharat-transliteration) (2024.1)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.10/dist-packages (from pydload->ai4bharat-transliteration) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pydload->ai4bharat-transliteration) (2.32.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->ai4bharat-transliteration) (1.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX->ai4bharat-transliteration) (24.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX->ai4bharat-transliteration) (3.20.3)\n",
            "Collecting tf2crf (from urduhack->ai4bharat-transliteration)\n",
            "  Downloading tf2crf-0.1.33-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-datasets~=3.1 (from urduhack->ai4bharat-transliteration)\n",
            "  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "INFO: pip is looking at multiple versions of urduhack to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting urduhack (from ai4bharat-transliteration)\n",
            "  Downloading urduhack-1.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading urduhack-1.0.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading urduhack-1.0.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading urduhack-1.0.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading urduhack-1.0.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "  Downloading urduhack-0.3.4-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting transformers~=2.10 (from urduhack->ai4bharat-transliteration)\n",
            "  Downloading transformers-2.11.0-py3-none-any.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urduhack (from ai4bharat-transliteration)\n",
            "  Downloading urduhack-0.3.3-py3-none-any.whl.metadata (7.2 kB)\n",
            "INFO: pip is still looking at multiple versions of urduhack to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading urduhack-0.3.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: tensorflow~=2.2 in /usr/local/lib/python3.10/dist-packages (from urduhack->ai4bharat-transliteration) (2.17.0)\n",
            "  Downloading urduhack-0.3.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading urduhack-0.2.7-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting regex (from fairseq->ai4bharat-transliteration)\n",
            "  Downloading regex-2019.12.20.tar.gz (679 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.8/679.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting urduhack (from ai4bharat-transliteration)\n",
            "  Downloading urduhack-0.2.6-py3-none-any.whl.metadata (5.6 kB)\n",
            "  Downloading urduhack-0.2.5-py3-none-any.whl.metadata (5.7 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading urduhack-0.2.4-py3-none-any.whl.metadata (5.7 kB)\n",
            "  Downloading urduhack-0.2.3-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading urduhack-0.2.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading urduhack-0.2.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading urduhack-0.1.4-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting omegaconf<2.1 (from fairseq->ai4bharat-transliteration)\n",
            "  Using cached omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.6 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl (from fairseq->ai4bharat-transliteration) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Using cached omegaconf-2.0.5-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.5 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl (from fairseq->ai4bharat-transliteration) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0mINFO: pip is looking at multiple versions of hydra-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting fairseq (from ai4bharat-transliteration)\n",
            "  Downloading fairseq-0.12.1.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfXc60V8fvLY",
        "outputId": "20c549fa-cb38-4078-b787-7c4e7385b3f3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/my_folder/vakyansh-tts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CaNVtbCaap-",
        "outputId": "9f173af3-6589-415c-8441-60bf7edb305b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1ZHma-OQ1z6C4gsp-jbESjuBGpeZ5cu6-/my_folder/vakyansh-tts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile final_app.py\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain.vectorstores import FAISS\n",
        "#from ai4bharat.transliteration import XlitEngine\n",
        "from mosestokenizer import MosesSentenceSplitter\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "from tts_infer.tts import TextToMel, MelToWav\n",
        "from tts_infer.num_to_word_on_sent import normalize_nums\n",
        "from scipy.io.wavfile import write\n",
        "import torch\n",
        "import os\n",
        "import openai\n",
        "import transformers\n",
        "import streamlit as st\n",
        "import re\n",
        "import numpy as np\n",
        "openai.api_key=\"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\"\n",
        "\n",
        "@st.cache_resource()\n",
        "def load_model():\n",
        "  English = {\n",
        "              'engine' : None,\n",
        "              'TextToMel': TextToMel(glow_model_dir='/content/drive/MyDrive/my_folder/vakyansh-tts/glow_ckp', device='cuda'),\n",
        "              'MelToWav': MelToWav(hifi_model_dir='/content/drive/MyDrive/my_folder/vakyansh-tts/hifi_ckp', device='cuda'),\n",
        "              'lang' : 'en'\n",
        "            }\n",
        "  # Hindi =  {   'engine' : XlitEngine(\"hi\", beam_width=7),\n",
        "  #              'TextToMel': TextToMel(glow_model_dir='/content/drive/MyDrive/models_testing/glow_ckp', device='cuda'),\n",
        "  #              'MelToWav': MelToWav(hifi_model_dir='/content/drive/MyDrive/models_testing/hifi_ckp', device='cuda'),\n",
        "  #              'lang': 'hi'\n",
        "  #           }\n",
        "  # Telugu = { 'engine' : XlitEngine(\"te\", beam_width=7),\n",
        "  #             'TextToMel': TextToMel(glow_model_dir='/content/drive/MyDrive/my_folder/glow', device='cuda'),\n",
        "  #             'MelToWav': MelToWav(hifi_model_dir='/content/drive/MyDrive/my_folder/hifi', device='cuda'),\n",
        "  #             'lang': 'te'\n",
        "  #           }\n",
        "  # Kannada = {   'engine' : XlitEngine(\"kn\", beam_width=7),\n",
        "  #               'TextToMel': TextToMel(glow_model_dir='/content/drive/MyDrive/models_testing/fe_glow', device='cuda'),\n",
        "  #               'MelToWav': MelToWav(hifi_model_dir='/content/drive/MyDrive/models_testing/ma_fe_hifi', device='cuda'),\n",
        "  #               'lang':'kn'\n",
        "  #           }\n",
        "  model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "  model_kwargs = {\"device\": \"cpu\"}\n",
        "  hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "  index = FAISS.load_local(\"/content/drive/MyDrive/models/Entrepreneurship_index_chatbot\", embeddings=hf)\n",
        "  #return index,English,Hindi,Telugu,Kannada\n",
        "  return index,English\n",
        "#index,English,Hindi,Telugu,Kannada=load_model()\n",
        "index,English=load_model()\n",
        "print('Finshed loading')\n",
        "\n",
        "# def transliteration(text,language):\n",
        "#     if language != 'English':\n",
        "#       if language == 'Hindi':\n",
        "#           text = Hindi['engine'].translit_sentence(text)['hi']\n",
        "#       elif language == 'Kannada':\n",
        "#           text = Kannada['engine'].translit_sentence(text)['kn']\n",
        "#       elif language == 'Telugu':\n",
        "#           text = Telugu['engine'].translit_sentence(text)['te']\n",
        "#      return text\n",
        "\n",
        "def context_generation(query):\n",
        "    context = index.similarity_search(query, k=1)[0].page_content\n",
        "    print(context)\n",
        "    return context\n",
        "\n",
        "def prompt_creation(context, query):\n",
        "  user_query_tempelate = f\"\"\"\n",
        "  Helloo! i have a question based on entrepreneurship,can you please provide me the Answer to the question based on the context given below in same format asked by user. provide a brief annswer.\n",
        "  Context:\n",
        "  {context}\n",
        "  Question:\n",
        "  {query}\n",
        "  Answer:\n",
        "  \"\"\"\n",
        "  return user_query_tempelate\n",
        "\n",
        "def split_sentences(paragraph, language):\n",
        "    if language == \"en\":\n",
        "        with MosesSentenceSplitter(language) as splitter:\n",
        "            return splitter([paragraph])\n",
        "    elif language in [\"hi\",\"te\",\"kn\"]:\n",
        "        return sentence_tokenize.sentence_split(paragraph, lang=language)\n",
        "\n",
        "def run_tts(text, lang):\n",
        "    text_num_to_word = normalize_nums(text, lang)\n",
        "    mel = TextToMel.generate_mel(text_num_to_word)\n",
        "    audio, sr =MelToWav.generate_wav(mel)\n",
        "    return (sr, audio)\n",
        "\n",
        "def run_tts_paragraph(text, language):\n",
        "    global TextToMel, MelToWav\n",
        "    if language==\"English\":\n",
        "       lang=English['lang']\n",
        "       TextToMel=English['TextToMel']\n",
        "       MelToWav=English['MelToWav']\n",
        "    elif language==\"Devanagari\":\n",
        "       lang=Hindi['lang']\n",
        "       TextToMel=Hindi['TextToMel']\n",
        "       MelToWav=Hindi['MelToWav']\n",
        "    elif language==\"Telugu\":\n",
        "       lang=Telugu['lang']\n",
        "       TextToMel=Telugu['TextToMel']\n",
        "       MelToWav=Telugu['MelToWav']\n",
        "    elif language==\"Kannada\":\n",
        "       lang=Kannada['lang']\n",
        "       TextToMel=Kannada['TextToMel']\n",
        "       MelToWav=Kannada['MelToWav']\n",
        "\n",
        "    audio_list = []\n",
        "    text = text.replace('।', '.')\n",
        "    r1=text.split(\"\\n\")\n",
        "    r2=' '.join(r1)\n",
        "    split_sentences_list = split_sentences(r2, lang)\n",
        "\n",
        "    for sent in split_sentences_list:\n",
        "        sr, audio = run_tts(sent, lang)\n",
        "        audio_list.append(audio)\n",
        "\n",
        "    concatenated_audio = np.concatenate([i for i in audio_list])\n",
        "    write(filename='/content/drive/MyDrive/models/temp_long.wav', rate=sr, data=concatenated_audio)\n",
        "    return (sr, concatenated_audio)\n",
        "\n",
        "\n",
        "def chatbot(query, language):\n",
        "    context= context_generation(query)\n",
        "    print(context)\n",
        "    print(\"starting to generate...\")\n",
        "    query_with_context =prompt_creation(context, query)\n",
        "    print(query_with_context)\n",
        "    response = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=query_with_context,\n",
        "        temperature = 1,\n",
        "        top_p = 0.5,\n",
        "        frequency_penalty = 1.2,\n",
        "        presence_penalty = 1.2,\n",
        "        max_tokens = 1024\n",
        "    )\n",
        "    print(\"Finished generation...\")\n",
        "    print(response)\n",
        "    return response['choices'][0]['text']\n",
        "\n",
        "## Front end dev:\n",
        "st.title(\"Welcome to my app\")\n",
        "language = st.selectbox(\"Select a Language\", ['Hindi', 'English', 'Telugu', 'Kannada'])\n",
        "question = st.text_input(\"Ask your question here\",placeholder= \"e.g. how can i become entrepreneur?\")\n",
        "if st.button(\"Submit\"):\n",
        "    response = chatbot(question, language)\n",
        "    st.success(response)\n",
        "    #transliteration(response,language)\n",
        "    _,audio = run_tts_paragraph(response, language)\n",
        "    st.audio('/content/drive/MyDrive/models/temp_long.wav')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIBuTjpBazAn",
        "outputId": "05400ec6-2619-47bc-f540-f3b939b3c8b5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting final_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run final_app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLKjRm__ooSZ",
        "outputId": "b6e31bfd-8497-451d-86d9-ce2875ea9584"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.91.112:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://hip-taxes-stick.loca.lt\n",
            "DEBUG:watchdog.observers.inotify_buffer:in-event <InotifyEvent: src_path=b'/content/drive/.shortcut-targets-by-id/1ZHma-OQ1z6C4gsp-jbESjuBGpeZ5cu6-/my_folder/vakyansh-tts/tts_infer/__pycache__/num_to_word_on_sent.cpython-310.pyc', wd=49, mask=IN_OPEN, cookie=0, name='num_to_word_on_sent.cpython-310.pyc'>\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWQD8v-dpTBn",
        "outputId": "359cf4d0-2bac-4898-b8ee-247e9796a9fc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.125.91.112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pzL2b7kseHvT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}